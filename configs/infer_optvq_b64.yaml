experiment:
    tokenizer_checkpoint: outputs/optvq_b64_stage2_shared-dinodisc/checkpoint-1000000/ema_model/pytorch_model.bin
    generator_checkpoint: outputs/optvq_b64_maskgit_shared_run1/checkpoint-300000/ema_model/pytorch_model.bin
    output_dir: "optvq_b64"
model:
    vq_model:
        quantize_mode: optvq
        codebook_size: 8192
        token_size: 32
        use_l2_norm: True
        commitment_cost: 0.25
        # vit arch
        vit_enc_model_size: "base"
        vit_dec_model_size: "base"
        vit_enc_patch_size: 16
        vit_dec_patch_size: 16
        num_latent_tokens: 64
        augment_ratio: 4  # number of parallel classification heads
        finetune_decoder: True
        semantic_guide: null # dinov2
        sem_loss_weight: 0.1
    
    generator:
        model_type: "ViT"
        hidden_size: 768
        num_hidden_layers: 24
        num_attention_heads: 16
        intermediate_size: 3072
        dropout: 0.1
        attn_drop: 0.1
        num_steps: 8
        class_label_dropout: 0.1
        image_seq_len: ${model.vq_model.num_latent_tokens}
        condition_num_classes: 1000

        # sampling hyper-params
        randomize_temperature: 11.0
        guidance_scale: 3.0
        guidance_decay: "linear"

dataset:
    preprocessing:
        crop_size: 256